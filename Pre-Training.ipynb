{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312514f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.data_loader import prepare_data_loader\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from accelerate import notebook_launcher\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from glob import glob\n",
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "# import psutil\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5da1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/logging.html\n",
    "\n",
    "# Setup logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e24463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl_file(data_dir):\n",
    "    PATH = os.path.join(data_dir)\n",
    "    EXT = \"*.pkl\"\n",
    "\n",
    "    pkl_files = []\n",
    "    for path, subdir, files in os.walk(PATH):\n",
    "        for file in glob(os.path.join(path, EXT)):\n",
    "            pkl_files.append(file)\n",
    "\n",
    "    print(\"pkl files:\", len(pkl_files))\n",
    "\n",
    "    return pkl_files\n",
    "\n",
    "\n",
    "def initialize_model(lr, momentum):\n",
    "    # https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Config.n_embd\n",
    "    with open(\"vocab/vocab.txt\", \"r\") as fh:\n",
    "        VOCAB_SIZE = len(fh.read().split(\"\\n\"))\n",
    "    config = {\n",
    "        \"vocab_size\": VOCAB_SIZE,  # Defines the number of different tokens that can be represented by the inputs_ids passed when calling GPT2Model or TFGPT2Model.\n",
    "        \"n_embd\": 512,  # Dimensionality of the embeddings and hidden states.\n",
    "        \"n_layer\": 4,  # Number of hidden layers in the Transformer encoder.\n",
    "        \"n_head\": 8,  # Number of attention heads for each attention layer in the Transformer encoder.\n",
    "        \"n_inner\": 2048,  # Dimensionality of the inner feed-forward layers.\n",
    "        \"use_cache\": False,  # TODO: !!\n",
    "    }\n",
    "    configuration = GPT2Config(**config)\n",
    "    model = GPT2LMHeadModel(configuration)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    return model, optimizer\n",
    "\n",
    "# https://huggingface.co/transformers/v3.5.1/custom_datasets.html\n",
    "# https://pytorch.org/docs/stable/data.html\n",
    "# https://pytorch.org/docs/stable/data.html#map-style-datasets\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    "# https://yizhepku.github.io/2020/12/26/dataloader.html\n",
    "class SequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx].detach().clone() for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "def save_model(model, model_dir, accelerator):\n",
    "    path = os.path.join(model_dir, \"model.pth\")\n",
    "    model = accelerator.unwrap_model(model)\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "\n",
    "def train(args):\n",
    "    # Init\n",
    "    use_cuda = args.num_gpus > 0\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    torch.manual_seed(args.random_seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(args.random_seed)\n",
    "\n",
    "    # Loading pkl files\n",
    "    pkl_files = load_pkl_file(args.data_dir)\n",
    "    pkl_files = pkl_files[:200]  # Limit to 200 files for testing\n",
    "\n",
    "    # # Retrieving information on running processes and system utilization (CPU, memory, disks, network, sensors)\n",
    "    # # https://pypi.org/project/psutil/\n",
    "    # process = psutil.Process(os.getpid())\n",
    "    # print(\"Memory Info:\", (process.memory_info().rss))  # in bytes\n",
    "\n",
    "    model, optimizer = initialize_model(\n",
    "        args.lr,\n",
    "        args.momentum,\n",
    "    )\n",
    "    # https://huggingface.co/docs/accelerate/accelerator\n",
    "    accelerator = Accelerator(\n",
    "        dispatch_batches=False, # If set to True, the dataloader prepared by the Accelerator is only iterated through on the main process and then the batches are split and broadcast to each process. Will default to True for DataLoader whose underlying dataset is an IterableDataset, False otherwise.\n",
    "        split_batches=False, # Whether or not the accelerator should split the batches yielded by the dataloaders across the devices.\n",
    "        fp16=True, # Mixed precision training.\n",
    "    )\n",
    "    model, optimizer = accelerator.prepare(model, optimizer)\n",
    "\n",
    "    print(\"Accelerator Info\")\n",
    "    print(accelerator.device)\n",
    "    print(accelerator.state)\n",
    "\n",
    "    device_idx = int(str(accelerator.state).split(\"\\n\")[2].split(\":\")[-1]) #TODO: !!\n",
    "    offset = math.ceil(\n",
    "        len(pkl_files) / int(str(accelerator.state).split(\"\\n\")[1].split(\":\")[-1]) #TODO !!\n",
    "    )\n",
    "    start_idx = device_idx * offset\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"from {start_idx} to {start_idx + offset}\")\n",
    "    \n",
    "        for pkl_idx, pkl_file in enumerate(pkl_files[start_idx : start_idx + offset]):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "\n",
    "                with open(pkl_file, 'rb') as fh:\n",
    "                    inputs = pickle.load(fh)\n",
    "\n",
    "                # Create dataset from tokenization\n",
    "                dataset = SequenceDataset(inputs)\n",
    "\n",
    "                # https://pytorch.org/docs/stable/data.html\n",
    "                loader = DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=args.batch_size,\n",
    "                    shuffle=True,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True, # Automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.\n",
    "                )\n",
    "\n",
    "                # https://huggingface.co/docs/accelerate/main/en/internal#accelerate.data_loader.prepare_data_loader\n",
    "                # Wraps a PyTorch DataLoader to generate batches for one of the processes only.\n",
    "                loader = prepare_data_loader(\n",
    "                    loader,\n",
    "                    split_batches=False,\n",
    "                    put_on_device=True,\n",
    "                    dispatch_batches=True,\n",
    "                )\n",
    "                end_time = time.time()\n",
    "                print(f\"Prepared pkl #{pkl_idx+1} of device #{device_idx} in {(end_time - start_time)} seconds\")\n",
    "            except Exception as e:\n",
    "                print(f\"CORRUPTED FILE: {pkl_file}\")\n",
    "                print(f\"EXCEPTION: {e}\")\n",
    "                continue\n",
    "\n",
    "            for batch_idx, batch in enumerate(loader):\n",
    "                start_time = time.time()\n",
    "\n",
    "                print(next(model.parameters()))\n",
    "                outputs = model(**batch)\n",
    "                print(outputs.loss, outputs.logits.shape)\n",
    "\n",
    "                accelerator.backward(outputs.loss) #TODO: !!\n",
    "                optimizer.step()\n",
    "                for param in model.parameters(): param.grad.zero_()\n",
    "                end_time = time.time()\n",
    "                print(f\"Trained batch #{batch_idx} of device #{device_idx} in {(end_time - start_time)} seconds\")\n",
    "\n",
    "            if pkl_idx % 2 == 0 and device_idx == 0:\n",
    "                print(f\"Saving model in pkl_idx #{pkl_idx} of device_idx #{device_idx}\")\n",
    "                accelerator.wait_for_everyone()\n",
    "                save_model(model, args.model_dir, accelerator)\n",
    "        # https://huggingface.co/docs/accelerate/accelerator\n",
    "        accelerator.wait_for_everyone() # to make sure all processes join that point before continuing (useful before a model save for instance).\n",
    "        save_model(model, args.model_dir, accelerator)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        metavar=\"BS\",\n",
    "        help=\"input batch size for training (default: 128)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        type=float,\n",
    "        default=5e-5,\n",
    "        metavar=\"LR\",\n",
    "        help=\"learning rate (default: 0.01)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--momentum\",\n",
    "        type=float,\n",
    "        default=0.9,\n",
    "        metavar=\"M\",\n",
    "        help=\"SGD momentum (default: 0.5)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--random-seed\",\n",
    "        type=int,\n",
    "        default=42,\n",
    "        metavar=\"RS\",\n",
    "        help=\"random seed (default: 42)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--backend\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\",\n",
    "    )\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument(\n",
    "        \"--hosts\",\n",
    "        type=list,\n",
    "        default=json.loads(os.environ[\"SM_HOSTS\"]),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--current-host\",\n",
    "        type=str,\n",
    "        default=os.environ[\"SM_CURRENT_HOST\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model-dir\",\n",
    "        type=str,\n",
    "        default=os.environ[\"SM_MODEL_DIR\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        type=str,\n",
    "        default=os.environ[\"SM_OUTPUT_DIR\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-dir\",\n",
    "        type=str,\n",
    "        default=os.environ[\"SM_CHANNEL_TRAINING\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-gpus\",\n",
    "        type=int,\n",
    "        default=os.environ[\"SM_NUM_GPUS\"],\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(\n",
    "        \",\".join([str(npgu) for npgu in range(args.num_gpus)])\n",
    "    )\n",
    "\n",
    "    print(f\"Instance with {args.num_gpus} GPUs in total\")\n",
    "\n",
    "    notebook_launcher(train, (args,), num_processes=args.num_gpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe65351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
